{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGBKYinqi3Ln",
    "outputId": "d8cd5a61-34ed-4a9d-8b5b-28a4b1944277",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:48:55.080397Z",
     "start_time": "2025-05-05T23:48:55.069501Z"
    }
   },
   "source": "# !pip install pyspark",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.classification import (\n",
    "    RandomForestClassifier, LogisticRegression, MultilayerPerceptronClassifier,\n",
    "    OneVsRest, LinearSVC, NaiveBayes\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark import keyword_only\n",
    "\n",
    "import math\n",
    "import pandas as pd"
   ],
   "metadata": {
    "id": "IyFTWkVT28Gz",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:48:55.556998Z",
     "start_time": "2025-05-05T23:48:55.130112Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup"
   ],
   "metadata": {
    "id": "2nNCvK4v2_9k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TEAM = 29\n",
    "spark = SparkSession.builder.appName(f\"{TEAM} - spark ML\").getOrCreate()"
   ],
   "metadata": {
    "id": "Nu6oeMIVk8AD",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:48:59.184473Z",
     "start_time": "2025-05-05T23:48:55.689260Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/06 02:48:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/06 02:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/06 02:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/06 02:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/06 02:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/06 02:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/06 02:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/06 02:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/06 02:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load data. TODO: replace with Hive"
   ],
   "metadata": {
    "id": "fGRa8X633CAk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "records_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"records.csv\")\n",
    ")\n",
    "\n",
    "stations_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"stations.csv\")\n",
    ")\n",
    "\n",
    "df = records_df.join(\n",
    "    stations_df.select(\"station_id\", \"latitude\", \"longitude\"),\n",
    "    on=\"station_id\",\n",
    "    how=\"left\"\n",
    ")"
   ],
   "metadata": {
    "id": "-x8EYxzhjLr7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "outputId": "95d24908-c705-4cd2-be41-0e1b39550fb7",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:10.388039Z",
     "start_time": "2025-05-05T23:48:59.311365Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 02:49:00 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rename and cast. TODO: pnut' Tmf to fix"
   ],
   "metadata": {
    "id": "3Urg7-Wf3LoU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if \"cmaq_organic_carbon\" in df.columns:\n",
    "    df = df.withColumn(\"cmaq_oc\", F.col(\"cmaq_organic_carbon\")).drop(\"cmaq_organic_carbon\")\n",
    "\n",
    "float_cols = [\n",
    "    \"airnow_ozone\", \"cmaq_ozone\", \"cmaq_no2\", \"cmaq_co\", \"cmaq_oc\",\n",
    "    \"pressure\", \"pbl\", \"temperature\", \"wind_speed\", \"wind_direction\",\n",
    "    \"radiation\"\n",
    "]\n",
    "\n",
    "for col in float_cols:\n",
    "    if col in df.columns:\n",
    "        df = df.withColumn(col, F.col(col).cast(\"float\"))"
   ],
   "metadata": {
    "id": "qRASnr5dlbiF",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:10.721608Z",
     "start_time": "2025-05-05T23:49:10.437244Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fill missing values with median"
   ],
   "metadata": {
    "id": "BGUWVSYO3TVF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for col in float_cols:\n",
    "    median = df.approxQuantile(col, [0.5], 0.25)[0]\n",
    "    df = df.fillna({col: median})"
   ],
   "metadata": {
    "id": "oEX7_CO03WGI",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:45.743553Z",
     "start_time": "2025-05-05T23:49:10.844630Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature engineering"
   ],
   "metadata": {
    "id": "ExqT67Xg3Y6E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SinCosTransformer(\n",
    "    Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable\n",
    "):\n",
    "    period = Param(\n",
    "        Params._dummy(),\n",
    "        \"period\",\n",
    "        \"Period for sin/cos\",\n",
    "        typeConverter=TypeConverters.toFloat,\n",
    "    )\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, period=None):\n",
    "        super(SinCosTransformer, self).__init__()\n",
    "        self._setDefault(period=1)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, period=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setPeriod(self, value):\n",
    "        return self._set(period=float(value))\n",
    "\n",
    "    def getPeriod(self):\n",
    "        return self.getOrDefault(self.period)\n",
    "\n",
    "    def setInputCol(self, value):\n",
    "        return self._set(inputCol=value)\n",
    "\n",
    "    def setOutputCol(self, value):\n",
    "        return self._set(outputCol=value)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        period = self.getPeriod()\n",
    "        inputCol = self.getInputCol()\n",
    "        outputCol = self.getOutputCol()\n",
    "\n",
    "        value = 2 * math.pi * F.col(inputCol) / period\n",
    "        dataset = dataset.withColumn(f\"{outputCol}_sin\", F.sin(value))\n",
    "        dataset = dataset.withColumn(f\"{outputCol}_cos\", F.cos(value))\n",
    "\n",
    "        return dataset"
   ],
   "metadata": {
    "id": "9gg6FU_V3dkQ",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:45.779750Z",
     "start_time": "2025-05-05T23:49:45.765920Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def add_ecef_columns(df, lat_col=\"latitude\", lon_col=\"longitude\", height_col=None):\n",
    "    a = 6378137.0\n",
    "    e2 = 6.6943799901377997e-3\n",
    "\n",
    "    df = df.withColumn(\"lat_rad\", F.radians(F.col(lat_col)))\n",
    "    df = df.withColumn(\"lon_rad\", F.radians(F.col(lon_col)))\n",
    "\n",
    "    if height_col is not None:\n",
    "        df = df.withColumn(\"ecef_height\", F.col(height_col))\n",
    "    else:\n",
    "        df = df.withColumn(\"ecef_height\", F.lit(0.0))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"ecef_N\",\n",
    "        F.lit(a) / F.sqrt(1 - F.lit(e2) * F.sin(F.col(\"lat_rad\")) * F.sin(F.col(\"lat_rad\")))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"ecef_x\",\n",
    "        (F.col(\"ecef_N\") + F.col(\"ecef_height\")) * F.cos(F.col(\"lat_rad\")) * F.cos(F.col(\"lon_rad\"))\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"ecef_y\",\n",
    "        (F.col(\"ecef_N\") + F.col(\"ecef_height\")) * F.cos(F.col(\"lat_rad\")) * F.sin(F.col(\"lon_rad\"))\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"ecef_z\",\n",
    "        (F.col(\"ecef_N\") * (1 - F.lit(e2)) + F.col(\"ecef_height\")) * F.sin(F.col(\"lat_rad\"))\n",
    "    )\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "id": "4cLbLI4Mt_sw",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:45.838781Z",
     "start_time": "2025-05-05T23:49:45.830504Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "df = add_ecef_columns(df)"
   ],
   "metadata": {
    "id": "T6r6FxJNuC3w",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:46.041087Z",
     "start_time": "2025-05-05T23:49:45.887208Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split data"
   ],
   "metadata": {
    "id": "b2jVC9Sb3i9M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed=42)"
   ],
   "metadata": {
    "id": "4EmOhJ263lQW",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:46.104587Z",
     "start_time": "2025-05-05T23:49:46.067160Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature pipeline"
   ],
   "metadata": {
    "id": "wKg9jGgm3oeN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "feature_cols = [\n",
    "    \"cmaq_ozone\", \"cmaq_oc\", \"pressure\", \"pbl\", \"temperature\",\n",
    "    \"cloud_fraction\", \"wind_speed\", \"ecef_x\", \"ecef_y\", \"ecef_z\",\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"linear_features\")\n",
    "scaler = StandardScaler(inputCol=\"linear_features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "\n",
    "sin_month = SinCosTransformer(inputCol=\"month\", outputCol=\"month\", period=12)\n",
    "sin_day = SinCosTransformer(inputCol=\"day\", outputCol=\"day\", period=31)\n",
    "sin_hour = SinCosTransformer(inputCol=\"hour\", outputCol=\"hour\", period=24)\n",
    "\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"scaled_features\", \"month_sin\", \"month_cos\", \"day_sin\", \"day_cos\",\n",
    "        \"hour_sin\", \"hour_cos\", \"ecef_x\", \"ecef_y\", \"ecef_z\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ],
   "metadata": {
    "id": "16OKgUfV3q-S",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:46.168890Z",
     "start_time": "2025-05-05T23:49:46.122713Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "pipeline = Pipeline(stages=[assembler, scaler, sin_month, sin_day, sin_hour, final_assembler])\n",
    "pipeline = pipeline.fit(train)\n",
    "\n",
    "train = pipeline.transform(train)\n",
    "test = pipeline.transform(test)"
   ],
   "metadata": {
    "id": "MhiQEUB14Y4D",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:57.007748Z",
     "start_time": "2025-05-05T23:49:46.226615Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Multiclass label transformation"
   ],
   "metadata": {
    "id": "UuYH5Heu33Cr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "categorize_radiation = F.when(F.col(\"radiation\") < 100, 0) \\\n",
    "            .when((F.col(\"radiation\") >= 100) & (F.col(\"radiation\") < 500), 1) \\\n",
    "            .otherwise(2)\n",
    "\n",
    "train = train.withColumn(\"label\", categorize_radiation)\n",
    "test = test.withColumn(\"label\", categorize_radiation)"
   ],
   "metadata": {
    "id": "BwAlQHj23xyM",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:57.200713Z",
     "start_time": "2025-05-05T23:49:57.129093Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Models and parameter grids (27 combinations each: 3 values x 3 hyperparameters)"
   ],
   "metadata": {
    "id": "2slSnFFb4mdR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_features = 16\n",
    "num_classes = 3"
   ],
   "metadata": {
    "id": "I4dfO0y7vLSk",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:57.225249Z",
     "start_time": "2025-05-05T23:49:57.221647Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "mlp = MultilayerPerceptronClassifier(labelCol=\"label\", featuresCol=\"features\", layers=[num_features, 32, num_classes])\n",
    "svc = LinearSVC(labelCol=\"label\", featuresCol=\"features\")\n",
    "svc_vs = OneVsRest(classifier=svc)\n",
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\", modelType=\"gaussian\")\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=100)\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "models = {\n",
    "    \"MLP\": (\n",
    "        mlp,\n",
    "        ParamGridBuilder()\n",
    "            .addGrid(mlp.stepSize, [0.01, 0.05, 0.1])  # Algorithm hyperparameter\n",
    "            .addGrid(mlp.blockSize, [32, 64, 128])  # Model hyperparameter\n",
    "            .addGrid(mlp.tol, [1e-4, 1e-3, 1e-2])  # Model hyperparameter\n",
    "            .build()\n",
    "    ),\n",
    "    \"NaiveBayes\": (\n",
    "        nb,\n",
    "        ParamGridBuilder()\n",
    "            .addGrid(nb.smoothing, [0.5, 1.0, 1.5])  # Algorithm hyperparameter\n",
    "            .addGrid(nb.thresholds, [\n",
    "                [1.0, 1.0, 1.0],         # Neutral (no weighting)\n",
    "                [1.5, 1.0, 0.5],         # Favor class 0\n",
    "                [0.5, 1.0, 1.5]          # Favor class 2\n",
    "            ])  # Model hyperparameter\n",
    "            .build()\n",
    "    ),\n",
    "    \"LogisticRegression\": (\n",
    "        lr,\n",
    "        ParamGridBuilder()\n",
    "            .addGrid(lr.regParam, [0.01, 0.1, 1.0])  # Algorithm hyperparameter\n",
    "            .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  # Model hyperparameter\n",
    "            .addGrid(lr.threshold, [0.3, 0.5, 0.7])  # Model hyperparameter\n",
    "            .build()\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        rf,\n",
    "        ParamGridBuilder()\n",
    "            .addGrid(rf.numTrees, [10, 50, 100])  # Algorithm hyperparameter\n",
    "            .addGrid(rf.maxDepth, [5, 10, 15])  # Model hyperparameter\n",
    "            .addGrid(rf.featureSubsetStrategy, [\"auto\", \"sqrt\", \"log2\"])  # Model hyperparameter\n",
    "            .build()\n",
    "    ),\n",
    "    \"SVM\": (\n",
    "        svc_vs,\n",
    "        ParamGridBuilder()\n",
    "            .addGrid(svc.regParam, [0.01, 0.1, 1.0])  # Algorithm hyperparameter\n",
    "            .addGrid(svc.tol, [1e-6, 1e-4, 1e-2])  # Model hyperparameter\n",
    "            .addGrid(svc.standardization, [True, False])  # Model hyperparameter\n",
    "            .build()\n",
    "    ),\n",
    "}\n"
   ],
   "metadata": {
    "id": "Vr_ll7Eq4nts",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:57.408974Z",
     "start_time": "2025-05-05T23:49:57.272191Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation"
   ],
   "metadata": {
    "id": "cDiDkQgK4uah"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "metrics = [\"accuracy\", \"f1\"]\n",
    "evaluators = {\n",
    "    m: MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=m)\n",
    "    for m in metrics\n",
    "}"
   ],
   "metadata": {
    "id": "g-6QOfCamGu4",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:49:57.466628Z",
     "start_time": "2025-05-05T23:49:57.436604Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train, evaluate and plot training dynamics"
   ],
   "metadata": {
    "id": "QdtwoDoq4zNx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test = test.select(\"features\", \"label\")\n",
    "train = train.select(\"features\", \"label\")\n",
    "train.show(10)"
   ],
   "metadata": {
    "id": "L3RBIuM_HnoW",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:50:00.609724Z",
     "start_time": "2025-05-05T23:49:57.503121Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[-0.5928154197071...|    0|\n",
      "|[-1.2557137044050...|    0|\n",
      "|[-0.7585399908816...|    0|\n",
      "|[-0.1785039917708...|    0|\n",
      "|[-1.4214382755795...|    0|\n",
      "|[-0.7585399908816...|    0|\n",
      "|[0.15294515057813...|    0|\n",
      "|[-1.5043005611668...|    0|\n",
      "|[-0.8414022764688...|    0|\n",
      "|[-0.7585399908816...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "train.write.mode(\"overwrite\").json(\"project/data/train\")\n",
    "test.write.mode(\"overwrite\").json(\"project/data/test\")"
   ],
   "metadata": {
    "id": "w3HqOVFrmpvY",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:50:32.763160Z",
     "start_time": "2025-05-05T23:50:00.716389Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "summary = []\n",
    "\n",
    "for name, (model, grid) in models.items():\n",
    "    print(f\"\\nTraining and evaluating {name}...\")\n",
    "    cv = CrossValidator(\n",
    "        estimator=model,\n",
    "        estimatorParamMaps=grid,\n",
    "        evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"),\n",
    "        numFolds=3\n",
    "    )\n",
    "\n",
    "    cv_model = cv.fit(train)\n",
    "    predictions = cv_model.transform(test)\n",
    "\n",
    "    print(f\"Metrics for {name}:\")\n",
    "    row = {\"Model\": name}\n",
    "    for m, e in evaluators.items():\n",
    "        score = e.evaluate(predictions)\n",
    "        row[m] = score\n",
    "        print(f\"{m}: {score:.4f}\")\n",
    "    summary.append(row)\n",
    "\n",
    "    predictions = predictions.select(\"label\", \"prediction\")\n",
    "    predictions.coalesce(1).write.mode(\"overwrite\").csv(f\"project/output/model_{name}_predictions.csv\")\n",
    "    cv_model.bestModel.write().mode(\"overwrite\").save(f\"project/models/model_{name}\")\n"
   ],
   "metadata": {
    "id": "B8mkE65d41HQ",
    "ExecuteTime": {
     "end_time": "2025-05-05T23:51:32.781208Z",
     "start_time": "2025-05-05T23:50:32.797406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 02:51:21 WARN BlockManager: Block rdd_219_6 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:25 ERROR Executor: Exception in task 6.0 in stage 48.0 (TID 311)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "25/05/06 02:51:27 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 6.0 in stage 48.0 (TID 311),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "25/05/06 02:51:28 WARN TaskSetManager: Lost task 6.0 in stage 48.0 (TID 311) (hadoop-01.uni.innopolis.ru executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\n",
      "25/05/06 02:51:29 ERROR TaskSetManager: Task 6 in stage 48.0 failed 1 times; aborting job\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_12 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_9 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_11 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_4 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_9 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_0 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_12 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_8 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_11 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_1 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_2 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_14 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_13 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_14 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_13 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_3 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_5 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_7 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_10 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_10 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN BlockManager: Putting block rdd_219_15 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/05/06 02:51:31 WARN BlockManager: Block rdd_219_15 could not be removed as it was not found on disk or in memory\n",
      "25/05/06 02:51:31 WARN TaskSetManager: Lost task 11.0 in stage 48.0 (TID 316) (hadoop-01.uni.innopolis.ru executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 6 in stage 48.0 failed 1 times, most recent failure: Lost task 6.0 in stage 48.0 (TID 311) (hadoop-01.uni.innopolis.ru executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/05/06 02:51:31 WARN TaskSetManager: Lost task 1.0 in stage 48.0 (TID 306) (hadoop-01.uni.innopolis.ru executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 6 in stage 48.0 failed 1 times, most recent failure: Lost task 6.0 in stage 48.0 (TID 311) (hadoop-01.uni.innopolis.ru executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/05/06 02:51:31 WARN TaskSetManager: Lost task 10.0 in stage 48.0 (TID 315) (hadoop-01.uni.innopolis.ru executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 6 in stage 48.0 failed 1 times, most recent failure: Lost task 6.0 in stage 48.0 (TID 311) (hadoop-01.uni.innopolis.ru executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/05/06 02:51:31 WARN TaskSetManager: Lost task 8.0 in stage 48.0 (TID 313) (hadoop-01.uni.innopolis.ru executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 6 in stage 48.0 failed 1 times, most recent failure: Lost task 6.0 in stage 48.0 (TID 311) (hadoop-01.uni.innopolis.ru executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/05/06 02:51:31 WARN TaskSetManager: Lost task 4.0 in stage 48.0 (TID 309) (hadoop-01.uni.innopolis.ru executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 6 in stage 48.0 failed 1 times, most recent failure: Lost task 6.0 in stage 48.0 (TID 311) (hadoop-01.uni.innopolis.ru executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/05/06 02:51:31 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 48.0 failed 1 times, most recent failure: Lost task 6.0 in stage 48.0 (TID 311) (hadoop-01.uni.innopolis.ru executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\n",
      "25/05/06 02:51:31 WARN TaskSetManager: Lost task 3.0 in stage 48.0 (TID 308) (hadoop-01.uni.innopolis.ru executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 6 in stage 48.0 failed 1 times, most recent failure: Lost task 6.0 in stage 48.0 (TID 311) (hadoop-01.uni.innopolis.ru executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2255/871411099.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2623/1312402865.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/05/06 02:51:31 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$4723/1690649073@284800e6 rejected from java.util.concurrent.ThreadPoolExecutor@5655f93c[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 312]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/05/06 02:51:31 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$4723/1690649073@136c034d rejected from java.util.concurrent.ThreadPoolExecutor@5655f93c[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 312]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/05/06 02:51:31 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$4723/1690649073@51d693f3 rejected from java.util.concurrent.ThreadPoolExecutor@5655f93c[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 312]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_32268/3970073718.py\", line 12, in <module>\n",
      "    cv_model = cv.fit(train)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/tuning.py\", line 847, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/usr/local/lib/python3.10/multiprocessing/pool.py\", line 870, in next\n",
      "    raise value\n",
      "  File \"/usr/local/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/tuning.py\", line 847, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 181, in deco\n",
      "    converted = convert_exception(e.java_exception)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 173, in convert_exception\n",
      "    return UnknownException(desc=e.toString(), stackTrace=stacktrace, cause=c)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 65, in __init__\n",
      "    self.cause = convert_exception(cause) if cause is not None else None\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 151, in convert_exception\n",
      "    elif is_instance_of(gw, e, \"org.apache.spark.SparkRuntimeException\"):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/java_gateway.py\", line 464, in is_instance_of\n",
      "    return gateway.jvm.py4j.reflection.TypeUtil.isInstanceOf(\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1664, in __getattr__\n",
      "    raise Py4JError(\"{0} does not exist in the JVM\".format(new_fqn))\n",
      "py4j.protocol.Py4JError: py4j.reflection does not exist in the JVM\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/team29/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j.reflection does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31m<class 'str'>\u001B[0m: (<class 'py4j.protocol.Py4JError'>, Py4JError('An error occurred while calling None.None'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 12\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTraining and evaluating \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m cv \u001B[38;5;241m=\u001B[39m CrossValidator(\n\u001B[1;32m      6\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m      7\u001B[0m     estimatorParamMaps\u001B[38;5;241m=\u001B[39mgrid,\n\u001B[1;32m      8\u001B[0m     evaluator\u001B[38;5;241m=\u001B[39mMulticlassClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      9\u001B[0m     numFolds\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m\n\u001B[1;32m     10\u001B[0m )\n\u001B[0;32m---> 12\u001B[0m cv_model \u001B[38;5;241m=\u001B[39m \u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m predictions \u001B[38;5;241m=\u001B[39m cv_model\u001B[38;5;241m.\u001B[39mtransform(test)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMetrics for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/tuning.py:847\u001B[0m, in \u001B[0;36mCrossValidator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    841\u001B[0m train \u001B[38;5;241m=\u001B[39m datasets[i][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcache()\n\u001B[1;32m    843\u001B[0m tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\n\u001B[1;32m    844\u001B[0m     inheritable_thread_target,\n\u001B[1;32m    845\u001B[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001B[1;32m    846\u001B[0m )\n\u001B[0;32m--> 847\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, metric, subModel \u001B[38;5;129;01min\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mimap_unordered(\u001B[38;5;28;01mlambda\u001B[39;00m f: f(), tasks):\n\u001B[1;32m    848\u001B[0m     metrics_all[i][j] \u001B[38;5;241m=\u001B[39m metric\n\u001B[1;32m    849\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m collectSubModelsParam:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/multiprocessing/pool.py:870\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m value\n\u001B[0;32m--> 870\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m value\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/multiprocessing/pool.py:125\u001B[0m, in \u001B[0;36mworker\u001B[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001B[0m\n\u001B[1;32m    123\u001B[0m job, i, func, args, kwds \u001B[38;5;241m=\u001B[39m task\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 125\u001B[0m     result \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m wrap_exception \u001B[38;5;129;01mand\u001B[39;00m func \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _helper_reraises_exception:\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/tuning.py:847\u001B[0m, in \u001B[0;36mCrossValidator._fit.<locals>.<lambda>\u001B[0;34m(f)\u001B[0m\n\u001B[1;32m    841\u001B[0m train \u001B[38;5;241m=\u001B[39m datasets[i][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcache()\n\u001B[1;32m    843\u001B[0m tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\n\u001B[1;32m    844\u001B[0m     inheritable_thread_target,\n\u001B[1;32m    845\u001B[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001B[1;32m    846\u001B[0m )\n\u001B[0;32m--> 847\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, metric, subModel \u001B[38;5;129;01min\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mimap_unordered(\u001B[38;5;28;01mlambda\u001B[39;00m f: \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, tasks):\n\u001B[1;32m    848\u001B[0m     metrics_all[i][j] \u001B[38;5;241m=\u001B[39m metric\n\u001B[1;32m    849\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m collectSubModelsParam:\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/util.py:342\u001B[0m, in \u001B[0;36minheritable_thread_target.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    341\u001B[0m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39msc()\u001B[38;5;241m.\u001B[39msetLocalProperties(properties)\n\u001B[0;32m--> 342\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/tuning.py:113\u001B[0m, in \u001B[0;36m_parallelFitTasks.<locals>.singleTask\u001B[0;34m()\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msingleTask\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m, Transformer]:\n\u001B[0;32m--> 113\u001B[0m     index, model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodelIter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001B[39;00m\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001B[39;00m\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001B[39;00m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;66;03m#  all nested stages and evaluators\u001B[39;00m\n\u001B[1;32m    118\u001B[0m     metric \u001B[38;5;241m=\u001B[39m eva\u001B[38;5;241m.\u001B[39mevaluate(model\u001B[38;5;241m.\u001B[39mtransform(validation, epm[index]))\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/base.py:98\u001B[0m, in \u001B[0;36m_FitMultipleIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     96\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo models remaining.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcounter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m index, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfitSingleModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/base.py:156\u001B[0m, in \u001B[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001B[0;34m(index)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfitSingleModel\u001B[39m(index: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m M:\n\u001B[0;32m--> 156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparamMaps\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(params, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m params:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 381\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:181\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 181\u001B[0m     converted \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_exception\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_exception\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m         \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m         \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m    185\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:173\u001B[0m, in \u001B[0;36mconvert_exception\u001B[0;34m(e)\u001B[0m\n\u001B[1;32m    167\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    168\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m  An exception was thrown from the Python worker. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    169\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease see the stack trace below.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m c\u001B[38;5;241m.\u001B[39mgetMessage()\n\u001B[1;32m    170\u001B[0m     )\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m PythonException(msg, stacktrace)\n\u001B[0;32m--> 173\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mUnknownException\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoString\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstackTrace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstacktrace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcause\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:65\u001B[0m, in \u001B[0;36mCapturedException.__init__\u001B[0;34m(self, desc, stackTrace, cause, origin)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstackTrace \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     61\u001B[0m     stackTrace\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stackTrace \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (SparkContext\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mutil\u001B[38;5;241m.\u001B[39mUtils\u001B[38;5;241m.\u001B[39mexceptionString(origin))\n\u001B[1;32m     64\u001B[0m )\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcause \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_exception\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcause\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m cause \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcause \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m origin \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m origin\u001B[38;5;241m.\u001B[39mgetCause() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcause \u001B[38;5;241m=\u001B[39m convert_exception(origin\u001B[38;5;241m.\u001B[39mgetCause())\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:151\u001B[0m, in \u001B[0;36mconvert_exception\u001B[0;34m(e)\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_instance_of(gw, e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjava.time.DateTimeException\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DateTimeException(origin\u001B[38;5;241m=\u001B[39me)\n\u001B[0;32m--> 151\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mis_instance_of\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43morg.apache.spark.SparkRuntimeException\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkRuntimeException(origin\u001B[38;5;241m=\u001B[39me)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_instance_of(gw, e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.spark.SparkUpgradeException\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/java_gateway.py:464\u001B[0m, in \u001B[0;36mis_instance_of\u001B[0;34m(gateway, java_object, java_class)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    461\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    462\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 464\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgateway\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpy4j\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreflection\u001B[49m\u001B[38;5;241m.\u001B[39mTypeUtil\u001B[38;5;241m.\u001B[39misInstanceOf(\n\u001B[1;32m    465\u001B[0m     param, java_object)\n",
      "File \u001B[0;32m~/BigDataProject/.venv10/lib/python3.10/site-packages/py4j/java_gateway.py:1664\u001B[0m, in \u001B[0;36mJavaPackage.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1661\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m JavaClass(\n\u001B[1;32m   1662\u001B[0m         answer[proto\u001B[38;5;241m.\u001B[39mCLASS_FQN_START:], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client)\n\u001B[1;32m   1663\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1664\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m does not exist in the JVM\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(new_fqn))\n",
      "\u001B[0;31mPy4JError\u001B[0m: py4j.reflection does not exist in the JVM"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "Export results for Superset"
   ],
   "metadata": {
    "id": "h9GuC72f5Cny"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pd.DataFrame(summary).to_csv(\"model_performance.csv\", index=False)"
   ],
   "metadata": {
    "id": "tNUN1AMS48qa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ZqwXAYp1tJvY"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
